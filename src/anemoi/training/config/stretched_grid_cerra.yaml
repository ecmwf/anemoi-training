# this is the CERRA config for anemoi-training with the stretched grid
defaults:
 - data: zarr
 - dataloader: native_grid
 - diagnostics: evaluation
 - hardware: slurm
 - graph: stretched_grid
 - model: graphtransformer
 - training: default
 - _self_

diagnostics:
  log:
    mlflow:
      enabled: True
      offline: False
      authentication: True
      experiment_name: 'aifs-stretch-cerra'
      log_model: False
      tracking_uri: 'https://mlflow.ecmwf.int'
    wandb:
      enabled: False
  plot:
    parameters:
      # - z_500
      - t_850
      - u_850
      - v_850
      - 2t
      - sp
      - msl
    callbacks:
      # Add plot callbacks here
      - _target_: anemoi.training.diagnostics.callbacks.plot.PlotLoss
        # group parameters by categories when visualizing contributions to the loss
        # one-parameter groups are possible to highlight individual parameters
        parameter_groups:
          moisture: [tp, cp, tcw]
#          sfc_wind: [10u, 10v]
      - _target_: anemoi.training.diagnostics.callbacks.plot.PlotSample
        sample_idx: ${diagnostics.plot.sample_idx}
        per_sample : 6
        parameters: ${diagnostics.plot.parameters}
        #Defining the accumulation levels for precipitation related fields and the colormap
        accumulation_levels_plot: [0, 0.05, 0.1, 0.25, 0.5, 1, 1.5, 2, 3, 4, 5, 6, 7, 100] # in mm
        cmap_accumulation: ["#ffffff", "#04e9e7", "#019ff4", "#0300f4", "#02fd02", "#01c501", "#008e00", "#fdf802", "#e5bc00", "#fd9500", "#fd0000", "#d40000", "#bc0000", "#f800fd"]
        precip_and_related_fields: ${diagnostics.plot.precip_and_related_fields}
      - _target_: anemoi.training.diagnostics.callbacks.plot.PlotHistogram
        sample_idx: ${diagnostics.plot.sample_idx}
        precip_and_related_fields: ${diagnostics.plot.precip_and_related_fields}
        parameters:
        - z_500
        - tp
        - 2t
#        - 10u
#        - 10v
  print_memory_summary: True

dataloader:
  num_workers:
    training: 2
    validation: 2
    test: 2
    predict: 2
  batch_size:
    training: 1
    validation: 1
    test: 1
    predict: 1

  dataset:
    cutout:
      - dataset: ${hardware.paths.data}/experimental/${hardware.files.dataset_lam}
      - dataset: ${hardware.paths.data}/stable/${hardware.files.dataset}
    adjust: all

  training:
    start: "1984-09-02"
    end: "2005-12-31"
  validation:
    start: "2006-01-01"
    end: "2007-12-31"
  test:
    start: "2008-01-01"
    end: "2008-06-29"

  limit_batches:
    training: 10
    validation: 10

hardware:
  num_gpus_per_model: 4
  paths:
    data: /home/mlx/ai-ml/datasets/
    grids: /home/mlx/ai-ml/grids/
    output: ${oc.env:HPCPERM}/anemoi/
    checkpoints: ${oc.env:SCRATCH}/anemoi/checkpoint/
    graph: ${oc.env:HPCPERM}/anemoi/graphs/
  files:
    dataset: aifs-ea-an-oper-0001-mars-o96-1979-2022-6h-v6.zarr
    dataset_lam: cerra-rr-an-oper-0001-mars-5p5km-1984-2008-6h-v1-smhi.zarr
    graph: test-anemoi-training_CERRA-2.pt
    warm_start: null #specific checkpoint to start from, defaults to last.ckpt

data:
  resolution: 5p5km
  forcing:
  - "cos_latitude"
  - "cos_longitude"
  - "sin_latitude"
  - "sin_longitude"
  - "cos_julian_day"
  - "cos_local_time"
  - "sin_julian_day"
  - "sin_local_time"
  - "insolation"
  - "lsm"
  diagnostic:
  - tp
  normalizer:
    default: "mean-std"
    std:
    - "tp"
    min-max:
    max:
    none:
    - "cos_latitude"
    - "cos_longitude"
    - "sin_latitude"
    - "sin_longitude"
    - "cos_julian_day"
    - "cos_local_time"
    - "sin_julian_day"
    - "sin_local_time"
    - "insolation"
    - "lsm"

model:
  num_channels: 512
  trainable_parameters:
    data: 0
    hidden: 0
    data2hidden: 0
    hidden2data: 0
    hidden2hidden: 0 # GNN and GraphTransformer Processor only

graph:
  save_graph_plots: True
  nodes:
    hidden:
      node_builder:
        lam_resolution: 9

training:
  run_id: '' #path to store the experiment in with output_base as root, null for random name, =fork_run_id to continue training in the same folder.
  fork_run_id: null #path to the experiment to fork from with output_base as root
  load_weights_only: False #loads entire model if False, loads only weights if True
  lr:
    rate: 5e-4  #8 * 0.625e-4
    min: 2.4e-6 #8 * 3e-7
    iterations: 150000

  validation_metrics:
    # loss class to initialise
    - _target_: anemoi.training.losses.mse.WeightedMSELoss
      #Â Scalars to include in loss calculation
      scalars: []
      ignore_nans: True
    - _target_: anemoi.training.losses.limitedarea.WeightedMSELossLimitedArea
      scalars: ['limited_area_mask']
      inside_lam: True
      wmse_contribution: False
      ignore_nans: True
