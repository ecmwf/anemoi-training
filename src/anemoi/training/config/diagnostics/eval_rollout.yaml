eval:
  enabled: True
  # use this to evaluate the model over longer rollouts, every so many validation batches
  rollout: 7
  frequency: 50

plot:
  enabled: True
  asynchronous: True
  frequency: 750
  sample_idx: 0
  per_sample: 6
  parameters:
  - z_500
  - t_850
  - u_850
  - v_850
  - 2t
  - 10u
  - 10v
  - tp


  sample_plot:
    #Defining the accumulation levels for precipitation related fields and the colormap
    accumulation_levels_plot: [0, 0.05, 0.1, 0.25, 0.5, 1, 1.5, 2, 3, 4, 5, 6, 7, 100] # in mm
    cmap_accumulation: ["#ffffff", "#04e9e7", "#019ff4", "#0300f4", "#02fd02", "#01c501", "#008e00", "#fdf802", "#e5bc00", "#fd9500", "#fd0000", "#d40000", "#bc0000", "#f800fd"]
    
    precip_and_related_fields: [tp]

  spectra:
    enabled: True
    parameters: 
      - z_500
      - tp
      - 2t
      - 10u
      - 10v

  loss_map:
    enabled: True

  loss_bar:
    enabled: True
    # group parameters by categories when visualizing contributions to the loss
    # one-parameter groups are possible to highlight individual parameters
    parameter_groups:
      moisture: [tp, cp, tcw]
      sfc_wind: [10u, 10v]

  power_spectrum: 
    enabled: True
    parameters:
      - z_500
      - tp
      - 2t
      - 10u
      - 10v

  learned_features:
    enabled: False

  histogram:
    enabled: True
    parameters: 
      - z_500
      - tp
      - 2t
      - 10u
      - 10v
  longrollout:
    enabled: false
    rollout: [60]
    frequency: 20 # every x epochs
  
  

debug:
  # this will detect and trace back NaNs / Infs etc. but will slow down training
  anomaly_detection: False

# activate the pytorch profiler (disable this in production)
# remember to also activate the tensorboard logger (below)
profiler: False


checkpoints:
  -
    type: "interval"
    kwargs:
      save_top_k: 6
      train_time_interval: null #minutes
      every_n_train_steps: null
      every_n_epochs: 1
      monitor: "step"
  -
    type: "performance"
    kwargs:
      monitor: "default"
      save_top_k: 3
      mode: "min"

# Leave empty if no early stopping rules required
early_stoppings:
  -
    patience: 20
    monitor: default
    mode: min

log:
  wandb:
    enabled: false
    offline: false
    log_model: false
    project: 'anemoi'
    entity: ???
    # logger options (these probably come with some overhead)
    gradients: false
    parameters: false
  tensorboard:
    enabled: False
  mlflow:
    enabled: false
    offline: false
    authentication: false
    log_model: false
    tracking_uri: ???
    experiment_name: 'anemoi-debug'
    project_name: 'anemoi'
    system: true
    terminal: true
    run_name: null # If set to null, the run name will be the a random UUID
    on_resume_create_child: true
    synchronous: False
  tensorboard:
    enabled: False
    run_name: ${diagnostics.log.mlflow.run_name}
    experiment_name: ${diagnostics.log.mlflow.experiment_name}
    project_name: null
    log_weights:
      enabled: False
      freq: 1
      interval: 'batch'
    log_gradients:
      enabled: False
      freq: 1
      # interval fixed to 'batch'
    log_clipped_gradients:
      enabled: False
      freq: 1
      # interval fixed to 'batch'
    log_preds:
      enabled: False
      freq: 1
    log_postproc_preds:
      enabled: False
      freq: 1
  interval: 100

enable_progress_bar: True
print_memory_summary: False

trainer_predict_step: true
