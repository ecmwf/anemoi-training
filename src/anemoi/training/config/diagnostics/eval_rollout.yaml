eval:
  enabled: True
  # use this to evaluate the model over longer rollouts, every so many validation batches
  rollout: 7
  frequency: 50
  lead_time_to_eval: [1, 4, 7]

plot:
  enabled: True
  asynchronous: True
  frequency: 750
  sample_idx: 0
  per_sample: 6
  parameters:
  - z_500
  - t_850
  - u_850
  - v_850
  - 2t
  - 10u
  - 10v
  - sp
  - tp
  - cp

  reconstructed_sample:
    enabled: True
    precipitation:
      #Defining the accumulation levels for precipitation related fields and the colormap
      accumulation_levels_plot: [0, 0.05, 0.1, 0.25, 0.5, 1, 1.5, 2, 3, 4, 5, 6, 7, 100] # in mm
      cmap_accumulation: ["#ffffff", "#04e9e7", "#019ff4", "#0300f4", "#02fd02", "#01c501", "#008e00", "#fdf802", "#e5bc00", "#fd9500", "#fd0000", "#d40000", "#bc0000", "#f800fd"]
      

  spectra:
    enabled: True
    parameters: 
      - z_500
      - tp
      - 2t
      - 10u
      - 10v

  loss_map:
    enabled: True

  loss_bar:
    enabled: True
    # group parameters by categories when visualizing contributions to the loss
    # one-parameter groups are possible to highlight individual parameters
    parameter_groups:
      moisture: [tp, cp, tcw]
      sfc_wind: [10u, 10v]

  power_spectrum: 
    enabled: True

  learned_features:
    enabled: False

  histogram:
    enabled: True
    parameters: 
      - z_500
      - tp
      - 2t
      - 10u
      - 10v


  
  

debug:
  # this will detect and trace back NaNs / Infs etc. but will slow down training
  anomaly_detection: False

# activate the pytorch profiler (disable this in production)
# remember to also activate the tensorboard logger (below)
profiler: False

# NOTE: Each checkpoint class needs to be able to be instatiated with a method as opposed to the complex logic currently in callbacks
# TODO: Maybe Convert this to use the instantiate method or at least clean up the method so a class handles the logic as opposed to code in the callback
# NOTE: make this more effecient so user does not have to state each save_frequency 
checkpoints:
  -
    type: "interval"
    kwargs:
      save_top_k: 6
      train_time_interval: null #minutes
      every_n_train_steps: null
      every_n_epochs: 1
      monitor: "step"
  -
    type: "performance"
    kwargs:
      monitor: "default"
      save_top_k: 3
      mode: "min"

# Leave empty if no early stopping rules required
early_stoppings:
  -
    patience: 20
    monitor: default
    mode: min

log:
  wandb:
    enabled: False
    offline: False
    log_model: False
    project: 'Anemoi'
    entity: ???
    # logger options (these probably come with some overhead)
    gradients: False
    parameters: False
  tensorboard:
    enabled: False
  mlflow:
    enabled: True
    offline: True
    authentication: False
    log_model: False
    tracking_uri: 'tracking_uri'
    experiment_name: 'forecasting'
    project_name: 'Anemoi'
    system: True
    terminal: True
    run_name: null # If set to null, the run name will be the a random UUID
    on_resume_create_child: True
    synchronous: True
  tensorboard:
    enabled: False
    run_name: ${diagnostics.log.mlflow.run_name}
    experiment_name: ${diagnostics.log.mlflow.experiment_name}
    project_name: null
    log_weights:
      enabled: False
      freq: 1
      interval: 'batch'
    log_gradients:
      enabled: False
      freq: 1
      # interval fixed to 'batch'
    log_clipped_gradients:
      enabled: False
      freq: 1
      # interval fixed to 'batch'
    log_preds:
      enabled: False
      freq: 1
    log_postproc_preds:
      enabled: False
      freq: 1
  interval: 100

enable_progress_bar: True
print_memory_summary: False
