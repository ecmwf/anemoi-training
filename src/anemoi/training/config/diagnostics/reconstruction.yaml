eval:
  enabled: True
  # use this to evaluate the model over longer rollouts, every so many validation batches
  rollout: 7
  frequency: 50

plot:
  enabled: False
  asynchronous: True
  frequency: 750
  sample_idx: 0
  per_sample: 6
  parameters:
  - z_500
  - t_850
  - u_850
  - v_850
  - 2t
  - 10u
  - 10v
  - sp
  - tp
  - cp

  
reconstructed_sample:
  enabled: True
  precipitation:
    #Defining the accumulation levels for precipitation related fields and the colormap
    accumulation_levels_plot: [0, 0.05, 0.1, 0.25, 0.5, 1, 1.5, 2, 3, 4, 5, 6, 7, 100] # in mm
    cmap_accumulation: ["#ffffff", "#04e9e7", "#019ff4", "#0300f4", "#02fd02", "#01c501", "#008e00", "#fdf802", "#e5bc00", "#fd9500", "#fd0000", "#d40000", "#bc0000", "#f800fd"]
    

spectra:
  enabled: True
  parameters: 
    - z_500
    - tp
    - 2t
    - 10u
    - 10v

loss_map:
  enabled: True

loss_bar:
  enabled: True
  # group parameters by categories when visualizing contributions to the loss
  # one-parameter groups are possible to highlight individual parameters
  parameter_groups:
    moisture: [tp, cp, tcw]
    sfc_wind: [10u, 10v]

spectral_loss: 
  enabled: True

learned_features:
  enabled: False

histogram:
  enabled: True
  parameters: 
    - z_500
    - tp
    - 2t
    - 10u
    - 10v


  
  

debug:
  # this will detect and trace back NaNs / Infs etc. but will slow down training
  anomaly_detection: False

# activate the pytorch profiler (disable this in production)
# remember to also activate the tensorboard logger (below)
profiler: False

# NOTE: Each checkpoint class needs to be able to be instatiated with a method as opposed to the complex logic currently in callbacks
# TODO: Maybe Convert this to use the instantiate method or at least clean up the method so a class handles the logic as opposed to code in the callback
checkpoints:
  -
    type: "interval"
    kwargs:
      save_top_k: 6
      train_time_interval: null #minutes
      every_n_train_steps: null
      every_n_epochs: 1
  -
    type: "performance"
    kwargs:
      monitor: "default"
      save_top_k: 3
      mode: "min"


early_stoppings:
  -
    patience: 20
    monitor: default
    mode: min

# NOTE: This should eventually be made into a class and exported to Anemoi-Experimental
experiment_manager:
  enabled: False
  invoked_from: null
  log_path: "experiment_mngr/log.csv"
  ckpt_restore_info:
    type: "performance" # 'performance' or 'interval'
    interval_type_to_restore_from: null # 'every_n_minutes', 'every_n_train_steps', 'every_n_epochs'
    perf_stage_to_restore_from: 'latest' # latest chooses the latest stage, for rollout this is rX.XXd e.g. r0.75d

log:
  tensorboard:
    enabled: False
    run_name: ${diagnostics.log.mlflow.run_name}
    experiment_name: ${diagnostics.log.mlflow.experiment_name}
    project_name: null
    log_weights:
      enabled: False
      freq: 1
      interval: 'batch'
    log_gradients:
      enabled: False
      freq: 1
      # interval fixed to 'batch'
    log_clipped_gradients:
      enabled: False
      freq: 1
      # interval fixed to 'batch'
    log_preds:
      enabled: False
      freq: 1
    log_postproc_preds:
      enabled: False
      freq: 1
  mlflow:
    enabled: True
    offline: True
    authentication: True
    log_model: False
    tracking_uri: blargghh
    experiment_name: 'reconstruction'
    project_name: 'Anemoi'
    system: True
    terminal: True
    run_name: null # If set to null, the run name will be the a random UUID
    on_resume_create_child: True
    synchronous: True
  wandb:
    enabled: False
    offline: False
    log_model: False
    project: 'Anemoi'
    entity: ???
    # logger options (these probably come with some overhead)
    gradients: False
    parameters: False
  interval: 100

enable_progress_bar: True
print_memory_summary: False
