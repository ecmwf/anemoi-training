# resume or fork a training from a checkpoint last.ckpt or specified in hardware.files.warm_start
run_id: null
fork_run_id: null
load_weights_only: null # only load model weights, do not restore optimiser states etc.

lightning_module:
  _target_: ??

# run in deterministic mode ; slows down
deterministic: False

# miscellaneous
float32_matmul_precision: high
precision: bf16-mixed

# multistep input
# 1 = single step scheme, X(t-1) used to predict X(t)
# k > 1: multistep scheme, uses [X(t-k), X(t-k+1), ... X(t-1)] to predict X(t)
# Deepmind use k = 2 in their model
multistep_input: 2

num_sanity_val_steps: 0  # start training directly

# gradient accumulation across K batches, K >= 1 (if K == 1 then no accumulation)
# the effective batch size becomes num-devices * batch_size * k
accum_grad_batches: 1

# clip gradients, 0 : don't clip, default algorithm: norm, alternative: value
gradient_clip:
  val: 32.
  algorithm: norm

# stochastic weight averaging
# https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/
swa:
  enabled: False
  lr: 1.e-4

# use ZeroRedundancyOptimizer ; saves memory for larger models
zero_optimizer: False

# dynamic rescaling of the loss gradient
# see https://arxiv.org/pdf/2306.06079.pdf, section 4.3.2
# don't enable this by default until it's been tested and proven beneficial
loss_gradient_scaling: False



max_epochs: null
max_steps: 150000

# TODO: Eventually move this to use the hydra initialize in order to test memory effecient optimizers

optimizer:
  weight_decay: 1e-8
  lr: 0.625e-4
  scale_by_gpus: False

scheduler:
  lr_min: 1.0e-7
  iterations: ${training.max_steps}
  warmup_t: 0.1 # TODO: ensure this is implemented correctly

# Changes in per-gpu batch_size should come with a rescaling of the local_lr
# in order to keep a constant global_lr
# global_lr = local_lr * num_gpus_per_node * num_nodes / gpus_per_model

feature_weighting:
  default: 1
  pl:
    q: 0.6 #1
    t: 6   #1
    u: 0.8 #0.5
    v: 0.5 #0.33
    w: 0.001
    z: 12  #1
  sfc:
    sp: 10
    10u: 0.1
    10v: 0.1
    2d: 0.5
    tp: 0.025
    cp: 0.0025
  inverse_tendency_variance_scaling: False

loss: 
  _target_: anemoi.training.losses.MSELoss
  _convert_: all


validation_metrics:
  variables:
    - all
    - group_all
    # - z_500
    # - t_850
    # - u_850
    # - v_850
  metrics: # list of metric classes
    -
      _target_: anemoi.training.losses.MSELoss
    -
      _target_: anemoi.training.losses.MAE
    -
      _target_: anemoi.training.losses.SkillScoreRatio #TODO: this needs to be calculated on denormalized values to have any meaning - relies on being calculated on non grouped values. Make sure the is_valid_compute flag indicates this should only be done when variable is not grouped????
    -
      _target_: anemoi.training.losses.KernelCRPS
      fair: True



pressure_level_scaler:
  _target_: anemoi.training.data.scaling.ReluPressureLevelScaler
  minimum: 0.2
  slope: 0.001

# Prediction strategy can be either 'tendency', 'residual' or 'state'
prediction_strategy: tendency

num_saniity_val_steps: 4